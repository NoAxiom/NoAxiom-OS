# NoAxiom调度器设计

## brief

我们的项目的调度器使用了CFS局部无锁调度队列的形式进行多核调度. 我觉得很帅!

## CFS设计

### cfs-tree

由于rust原生支持了`BTreeSet`, 并且没有比较好的对于红黑树的支持, 我们就使用`BTreeSet`进行了cfs的维护. 内部节点主要使用`sched_entity`中的vruntime进行比较来进行最小节点的维护.

此外还在树节点中维护了tid来合并相同的被唤醒的任务, 防止任务的重复唤醒. 所以说, 这是一个set行为, 而不是multiset.

### sched_entity

我们使用`sched_entity`进行cfs内部节点的维护, 通过vruntime来表示一个虚拟的带权运行时间. 这使得我们的任务可以支持优先级的更改. 此外还使用了各类除法优化来优化load计算时的时钟周期开销.

此外, 我们在`sched_entity`的维护当中使用了`usize`相减后转为`isize`的trick来保证整数溢出不会影响比较的正确性.
p.s. 这个trick在Sleep的维护当中也大量被应用.

## 多核调度器设计

我们的目标是实现一个多核无锁的调度器. 这确实有点难.

### hartid绑定

首先我们假定某一进程固定运行在某一hart上, 通过绑定hart的方式给定了进程在无核心迁移情况下的cache数据一致性. 在这种情况下可以认为进程就是运行在单核上.

但是一旦涉及到进程间通信 / 跨核心的唤醒 / 负载均衡的时候, 就必须要通过锁机制进行通知, 并修改相关进程运行的hartid.

由于目标是大部分情况下的无锁调度, 我们需要尽量减少锁的行为, 换言之, 需要尽量通过原子标记位进行调度请求 / 负载均衡的通知.

### 多核负载均衡

我们尝试使用activate balance, 也就是高负载核心检测低负载核心发出的调度请求, 然后主动访问全局锁pop task的方式, 进行数据的维护.

对于核心负载, 我们定义`local_load = sum{ cfs_load }`

对于全局负载, 我们定义`all_load`为所有核心的`local_load`, 并通过原子指令进行全局负载的一致性维护.

#### 负载均衡策略

我们令全局load为`all_load`, 以双核为例, 检测到某一核心占据了全局2/3以上任务的时候, 认为它超过了最大负载, 就必须弹出任务了.

同样的, 假如某一核心占据了全局1/3以下任务的时候, 它会主动发出调度请求, 有可能就有高负载核心检测到调度请求, 将任务push进来.

特别地, 假如某一核心没有任务可以调度, 那么它会直接进入idle状态, 直到高负载核心向它发送ipi为止才会继续进行任务的执行. 这样可以避免比较激烈的数据竞争, 否则空闲核心会持续地尝试对于全局数据进行访问, 这会影响到其他核心获取数据.

### mailbox

我们通过带锁的mailbox进行数据通信. 具体而言, 我们给每个核心都开了一个per-CPU的带spinlock的任务队列, 用于接受其他核心向它发送的等待调度的任务.

mailbox被push有以下两种情况:

1. 检测到当前被唤醒进程应该运行的hart与当前hart不同时
2. 发生load_balance, 高负载核心将任务从本地队列中弹出, push到低负载核心的mailbox

为了尽量避免mailbox的带锁访问, 我们设置了一个empty的原子标记位, 只有当原子读到当前empty为非空的时候, 才会尝试对于mailbox上锁并进行获取mailbox中的任务.

### sched_req

低负载核心向高负载核心发送调度申请, 并不是通过ipi完成的, 而是通过一个异步的调度请求完成. 只有当高负载核心从用户进程执行当中返回调度控制流的时候, 才会尝试相应低负载核心的请求.

这个请求我们通过一个`AtomicUsize`维护的bitmap进行维护, 具体而言, 每次都会先尝试读取bitmap的值, 假如检测到非0, 则尝试对于每个核心的请求进行轮询.
todo: 这里可以使用ticket机制来防止饥饿.

### IPI

*听说*IPI的开销在qemu上比较大, 所以我们尽量避免了IPI的产生.

目前而言, 仅有高负载核心相应IPI的时候, 会向低负载核心发送IPI, 使其尽可能地早一点响应弹出的任务.

由于还涉及到了tlb_shoot_down等其他ipi的维护, 我们通过自定义的IPI_info进行具体ipi类型的定义.
